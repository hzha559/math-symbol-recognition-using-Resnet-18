{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import cv2\n",
    "from skimage import io\n",
    "import fs\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "cuda = torch.cuda.is_available()\n",
    "import time\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "from mlxtend.data import loadlocal_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DrivingDataset(Dataset):\n",
    "    def __init__(self,data_dir, input_w=224, input_h=224,is_train=True,transform=None):\n",
    "        if is_train==False:\n",
    "            threshold=51#use 50 from each class as validation\n",
    "        else:\n",
    "            threshold=300#300 as training\n",
    "        namelist = [[] for i in range(15)]\n",
    "        \n",
    "        self.data_filenames = []\n",
    "        self.data_ids = []\n",
    "        self.is_train=is_train\n",
    "\n",
    "        self.data_root=fs.open_fs(data_dir)\n",
    "        self.transform = transform\n",
    "        for p in self.data_root.walk.files(filter=[\"*.jpg\",\"*.png\"]):\n",
    "            filename=data_dir+p\n",
    "            if is_train==True:#temporary bug\n",
    "                if \"zero\"  in filename:\n",
    "                    if len(namelist[0])<threshold:\n",
    "                        self.data_filenames.append(filename)\n",
    "                        self.data_ids.append(0)\n",
    "                        namelist[0].append(1)\n",
    "                elif \"one\" in filename:\n",
    "                    if len(namelist[1])<threshold:\n",
    "                        self.data_filenames.append(filename)\n",
    "                        self.data_ids.append(1)\n",
    "                        namelist[1].append(1)\n",
    "                elif \"two\" in filename:\n",
    "                    if len(namelist[2])<threshold:\n",
    "                        self.data_filenames.append(filename)\n",
    "                        self.data_ids.append(2)\n",
    "                        namelist[2].append(1)\n",
    "                elif \"three\" in filename:\n",
    "                    if len(namelist[3])<threshold:\n",
    "                        self.data_filenames.append(filename)\n",
    "                        self.data_ids.append(3)\n",
    "                        namelist[3].append(1)\n",
    "                elif \"four\" in filename:\n",
    "                    if len(namelist[4])<threshold:\n",
    "                        self.data_filenames.append(filename)\n",
    "                        self.data_ids.append(4)\n",
    "                        namelist[4].append(1)\n",
    "                elif \"five\" in filename:\n",
    "                    if len(namelist[5])<threshold:\n",
    "                        self.data_filenames.append(filename)\n",
    "                        self.data_ids.append(5)\n",
    "                        namelist[5].append(1)\n",
    "                elif \"six\" in filename:\n",
    "                    if len(namelist[6])<threshold:\n",
    "                        self.data_filenames.append(filename)\n",
    "                        self.data_ids.append(6)\n",
    "                        namelist[6].append(1)\n",
    "                elif \"seven\" in filename:\n",
    "                    if len(namelist[7])<threshold:\n",
    "                        self.data_filenames.append(filename)\n",
    "                        self.data_ids.append(7)\n",
    "                        namelist[7].append(1)\n",
    "                elif \"eight\" in filename:\n",
    "                    if len(namelist[8])<threshold:\n",
    "                        self.data_filenames.append(filename)\n",
    "                        self.data_ids.append(8)\n",
    "                        namelist[8].append(1)\n",
    "                elif \"nine\" in filename:\n",
    "                    if len(namelist[9])<threshold:\n",
    "                        self.data_filenames.append(filename)\n",
    "                        self.data_ids.append(9)\n",
    "                        namelist[9].append(1)\n",
    "                elif \"plus\" in filename:\n",
    "                    if len(namelist[10])<threshold:\n",
    "                        self.data_filenames.append(filename)\n",
    "                        self.data_ids.append(10)\n",
    "                        namelist[10].append(1)\n",
    "                elif \"minus\" in filename:\n",
    "                    if len(namelist[11])<threshold:\n",
    "                        self.data_filenames.append(filename)\n",
    "                        self.data_ids.append(11)\n",
    "                        namelist[11].append(1)\n",
    "\n",
    "                elif \"times\" in filename:\n",
    "                    if len(namelist[12])<threshold:\n",
    "                        self.data_filenames.append(filename)\n",
    "                        self.data_ids.append(12)\n",
    "                        namelist[12].append(1)\n",
    "                elif \"div\" in filename:\n",
    "                    if len(namelist[13])<threshold:\n",
    "                        self.data_filenames.append(filename)\n",
    "                        self.data_ids.append(13)\n",
    "                        namelist[13].append(1)\n",
    "                elif \"equal\" in filename:\n",
    "                    if len(namelist[14])<threshold:\n",
    "                        self.data_filenames.append(filename)\n",
    "                        self.data_ids.append(14)\n",
    "                        namelist[14].append(1)\n",
    "            else:\n",
    "                self.data_filenames.append(filename)\n",
    "                self.data_ids.append(0)\n",
    "        \n",
    "        \n",
    "        # print(self.data_filenames)\n",
    "        #print(self.data_ids)\n",
    "        print(len(self.data_ids))\n",
    "\n",
    "        #self.input_w = input_w\n",
    "        #self.input_h = input_h\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Grey(i, j) = 0.299 × R(i, j) + 0.587 × G(i, j) + 0.114 × B(i, j)\"\"\"\n",
    "\n",
    "        img_path = self.data_filenames[item]\n",
    "        #print(img_path)\n",
    "        target = self.data_ids[item]\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        target = np.array([target], dtype=np.long)\n",
    "        target = torch.from_numpy(target)\n",
    "        \n",
    "        return image,target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_filenames)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#新的transform，通过padding保证不畸变图片,ratio是需要设置的最终图片长宽比\n",
    "class ratio_crop(object):\n",
    "    def __init__(self, ratio=1.0):\n",
    "        self.ratio = ratio\n",
    "    def __call__(self, images):\n",
    "            ratio=1.0\n",
    "            #for img in images:\n",
    "            #print(images.shape)\n",
    "            w=images.shape[1]\n",
    "            h=images.shape[0]\n",
    "            aspect_ratio=float(w)/float(h)\n",
    "            #print(images.shape,aspect_ratio)\n",
    "            if aspect_ratio==ratio:\n",
    "                a=1\n",
    "            elif aspect_ratio>ratio:\n",
    "                dif = np.abs(w  - h)\n",
    "                pad1, pad2 = int(dif // 2), int(dif - dif // 2)\n",
    "                pad = ((0, 0),(pad1, pad2) ,(0, 0))\n",
    "                images = np.pad(images, pad, \"constant\", constant_values=255)\n",
    "                #input_img = cv2.resize(input_x, (inputwidth, inputheight))\n",
    "            else:\n",
    "                # padding w\n",
    "                dif = np.abs(h  - w)\n",
    "                pad1, pad2 = int(dif // 2), int(dif - dif // 2)\n",
    "                pad = ((0, 0),(pad1, pad2),(0, 0))\n",
    "                images = np.pad(images, pad, \"constant\", constant_values=255)\n",
    "                #input_img = cv2.resize(input_x, (inputwidth, inputheight))\n",
    "            return images\n",
    "        \n",
    "transform = transforms.Compose([\n",
    "            ratio_crop(1.0),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((28,28), interpolation=2),\n",
    "            transforms.Pad(5, fill=255, padding_mode='constant'),\n",
    "            #transforms.RandomResizedCrop(56, scale=(0.7, 1.0)),\n",
    "            transforms.Resize((56,56), interpolation=2),\n",
    "            \n",
    "            #transforms.RandomHorizontalFlip(0.2), \n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/zhaoh/Downloads/FYP/UI/master/sliced/'\n",
    "for f in os.listdir(path):\n",
    "    name=os.path.join(path+str(f))\n",
    "    if \"jpg\" in name:\n",
    "        im = cv2.imread(name)\n",
    "        if im.all()!=0:\n",
    "            os.remove(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "data_dir=path\n",
    "#train_set = DrivingDataset(data_dir=\"C:/Users/zhaoh/Downloads/FYP/dataset/train/\", is_train=True,transform=transform) \n",
    "#val_set = DrivingDataset(data_dir=\"C:/Users/zhaoh/Downloads/FYP/dataset/test/\", is_train=False,transform=transform) \n",
    "test_set = DrivingDataset(path,is_train=False, transform=transform) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "#n_workers = multiprocessing.cpu_count()\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=50,\n",
    "                                          shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for dimension 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a3a68aa381e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gray\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 5 is out of bounds for dimension 0 with size 5"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#take a look of training set\n",
    "image_batch,label= next(iter(testloader))\n",
    "for i in range(20): \n",
    "    img = (image_batch[i].numpy())\n",
    "    plt.subplot(2,4,(i%8+1))\n",
    "    plt.imshow(img[0],cmap = \"gray\")\n",
    "    if i%8==0:\n",
    "        plt.show()\n",
    "    \n",
    "print(img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the network below\n",
    "####################################\n",
    "#######################################\n",
    "###########################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.hub import load_state_dict_from_url \n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
    "           'wide_resnet50_2', 'wide_resnet101_2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
    "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    __constants__ = ['downsample']\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    __constants__ = ['downsample']\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)###########################3 input channels\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # Allow for accessing forward method in a inherited class\n",
    "    forward = _forward\n",
    "\n",
    "\n",
    "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def resnet152(pretrained=True, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-152 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "def resnet18(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-49f3991473cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/zhaoh/Downloads/FYP/modelnopad1.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcriterion_hinge\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiMarginLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    571\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "model = torch.load('C:/Users/zhaoh/Downloads/FYP/modelnopad1.pkl')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion_hinge=nn.MultiMarginLoss()\n",
    "learning_rate = 1e-4\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = model.conv1.in_channels\n",
    "\n",
    "#model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "        #Predicted=[]\n",
    "        for data, target in (test_loader):\n",
    "            \n",
    "            data = data.float().to(device)\n",
    "            #print(data.shape)\n",
    "            target=target.squeeze(1)\n",
    "            target = target.long().to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            #print(predicted.shape)\n",
    "            #Predicted.append(predicted)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, target).detach()\n",
    "            loss+=criterion_hinge(outputs, target).detach()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Testing Loss: ', running_loss)\n",
    "        print('Testing Accuracy: ', acc, '%')\n",
    "        return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"C:/Users/zhaoh/Downloads/FYP/deeplearning new/new-number-identification/crop/\"\n",
    "#\"C:/Users/zhaoh/Downloads/FYP/dataset/test/\"\n",
    "'''path='C:/Users/zhaoh/Downloads/FYP/dataset/test/'\n",
    "i=0\n",
    "for f in os.listdir(path):\n",
    "    name=os.path.join(path+str(f))\n",
    "    if \"jpg\" in name:\n",
    "        im = cv2.imread(name)\n",
    "        i+=1\n",
    "        flip = 255-im\n",
    "        cv2.imwrite(name,flip)\n",
    "print(i)\n",
    "'''\n",
    "#test_set = DrivingDataset(data_dir=\"C:/Users/zhaoh/Downloads/FYP/dataset/test/\", is_train=False,transform=transform)\n",
    "#testloader = torch.utils.data.DataLoader(test_set, batch_size=5,\n",
    "                                          #shuffle=True, num_workers=0)\n",
    "for i,j in testloader:\n",
    "    #i=255-i\n",
    "    print(i.shape)\n",
    "    for k in range(4):\n",
    "        img = np.moveaxis(i[k].numpy(),0,2)\n",
    "        #print(img.shape)\n",
    "        plt.imshow(img[:,:,0],cmap='gray')\n",
    "        plt.show()\n",
    "    i=i.reshape(-1,3,56,56)\n",
    "    outputs = model(i.to(device))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    #outputs = model(cell0.to(device))\n",
    "    #_, predicted = torch.max(outputs.data, 1)\n",
    "    #plt.show()\n",
    "    print(\"prediction\",predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
