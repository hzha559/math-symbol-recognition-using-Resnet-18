{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import cv2\n",
    "from skimage import io\n",
    "import fs\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import torch.nn as nn\n",
    "cuda = torch.cuda.is_available()\n",
    "import time\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/zhaoh/Downloads/FYP/dataset/previous train/'#segment the training images from the ipad\n",
    "i=0\n",
    "for f in os.listdir(path):\n",
    "    name=os.path.join(path+str(f))\n",
    "    if \"png\" in name:\n",
    "        im = cv2.imread(name)\n",
    "        plt.imshow(im)\n",
    "        #plt.show()\n",
    "        im1=im[300:435,420:575]#vertical 135,horizontal 155\n",
    "        im2=im[300:435,580:735]\n",
    "        im3=im[300:435,740:895]\n",
    "        im4=im[300:435,900:1055]\n",
    "        im5=im[300:435,1060:1210]\n",
    "        im6=im[300:435,1215:1370]\n",
    "        \n",
    "        im7=im[440:575,420:575]\n",
    "        im8=im[440:575,580:735]\n",
    "        im9=im[440:575,740:895]\n",
    "        im10=im[440:575,900:1055]\n",
    "        im11=im[440:575,1060:1210]\n",
    "        im12=im[440:575,1215:1370]\n",
    "        \n",
    "        im13=im[580:715,420:575]\n",
    "        im14=im[580:715,580:735]\n",
    "        im15=im[580:715,740:895]\n",
    "        im16=im[580:715,900:1055]\n",
    "        im17=im[580:715,1060:1210]\n",
    "        im18=im[580:715,1215:1370]\n",
    "        \n",
    "        im19=im[720:855,420:575]\n",
    "        im20=im[720:855,580:735]\n",
    "        im21=im[720:855,740:895]\n",
    "        im22=im[720:855,900:1055]\n",
    "        im23=im[720:855,1060:1210]\n",
    "        im24=im[720:855,1215:1370]\n",
    "        \n",
    "        im25=im[860:995,420:575]\n",
    "        im26=im[860:995,580:735]\n",
    "        im27=im[860:995,740:895]\n",
    "        im28=im[860:995,900:1055]\n",
    "        im29=im[860:995,1060:1210]\n",
    "        im30=im[860:995,1215:1370]\n",
    "        \n",
    "        im31=im[1000:1140,420:575]\n",
    "        im32=im[1000:1140,580:735]\n",
    "        im33=im[1000:1140,740:895]\n",
    "        im34=im[1000:1140,900:1055]\n",
    "        im35=im[1000:1140,1060:1210]\n",
    "        im36=im[1000:1140,1215:1370]\n",
    "        \n",
    "        im37=im[1145:1280,420:575]\n",
    "        im38=im[1145:1280,580:735]\n",
    "        im39=im[1145:1280,740:895]\n",
    "        im40=im[1145:1280,900:1055]\n",
    "        im41=im[1145:1280,1060:1210]\n",
    "        im42=im[1145:1280,1215:1370]\n",
    "        \n",
    "        im43=im[1285:1420,420:575]\n",
    "        im44=im[1285:1420,580:735]\n",
    "        im45=im[1285:1420,740:895]\n",
    "        im46=im[1285:1420,900:1055]\n",
    "        im47=im[1285:1420,1060:1210]\n",
    "        im48=im[1285:1420,1215:1370]\n",
    "        \n",
    "        im49=im[1425:1560,420:575]\n",
    "        im50=im[1425:1560,580:735]\n",
    "        im51=im[1425:1560,740:895]\n",
    "        im52=im[1425:1560,900:1055]\n",
    "        im53=im[1425:1560,1060:1210]\n",
    "        im54=im[1425:1560,1215:1370]\n",
    "        \n",
    "        im55=im[1980:2117,420:575]\n",
    "        im56=im[1980:2117,580:735]\n",
    "        im57=im[1980:2117,740:895]\n",
    "        im58=im[1980:2117,900:1055]\n",
    "        im59=im[1980:2117,1060:1210]\n",
    "        im60=im[1980:2117,1215:1370]\n",
    "        #im5=im[870:990,420:575]\n",
    "        '''\n",
    "        plt.imshow(im55)\n",
    "        plt.show()\n",
    "        plt.imshow(im56)\n",
    "        plt.show()\n",
    "        plt.imshow(im57)\n",
    "        plt.show()\n",
    "        plt.imshow(im58)\n",
    "        plt.show()\n",
    "        plt.imshow(im59)\n",
    "        plt.show()\n",
    "        plt.imshow(im60)\n",
    "        plt.show()\n",
    "        '''\n",
    "        i+=1\n",
    "        #im=255-im\n",
    "        #flip = cv2.flip(im, 1)\n",
    "        \n",
    "        cv2.imwrite(path+'one/'+str(np.random.randint(100000,size=1).item())+'.jpg',im1)\n",
    "        cv2.imwrite(path+'one/'+str(np.random.randint(100000,size=1).item())+'.jpg',im2)\n",
    "        cv2.imwrite(path+'one/'+str(np.random.randint(100000,size=1).item())+'.jpg',im3)\n",
    "        cv2.imwrite(path+'one/'+str(np.random.randint(100000,size=1).item())+'.jpg',im4)\n",
    "        cv2.imwrite(path+'one/'+str(np.random.randint(100000,size=1).item())+'.jpg',im5)\n",
    "        cv2.imwrite(path+'one/'+str(np.random.randint(100000,size=1).item())+'.jpg',im6)\n",
    "        \n",
    "        cv2.imwrite(path+'two/'+str(np.random.randint(100000,size=1).item())+'.jpg',im7)\n",
    "        cv2.imwrite(path+'two/'+str(np.random.randint(100000,size=1).item())+'.jpg',im8)\n",
    "        cv2.imwrite(path+'two/'+str(np.random.randint(100000,size=1).item())+'.jpg',im9)\n",
    "        cv2.imwrite(path+'two/'+str(np.random.randint(100000,size=1).item())+'.jpg',im10)\n",
    "        cv2.imwrite(path+'two/'+str(np.random.randint(100000,size=1).item())+'.jpg',im11)\n",
    "        cv2.imwrite(path+'two/'+str(np.random.randint(100000,size=1).item())+'.jpg',im12)\n",
    "        \n",
    "        cv2.imwrite(path+'three/'+str(np.random.randint(100000,size=1).item())+'.jpg',im13)\n",
    "        cv2.imwrite(path+'three/'+str(np.random.randint(100000,size=1).item())+'.jpg',im14)\n",
    "        cv2.imwrite(path+'three/'+str(np.random.randint(100000,size=1).item())+'.jpg',im15)\n",
    "        cv2.imwrite(path+'three/'+str(np.random.randint(100000,size=1).item())+'.jpg',im16)\n",
    "        cv2.imwrite(path+'three/'+str(np.random.randint(100000,size=1).item())+'.jpg',im17)\n",
    "        cv2.imwrite(path+'three/'+str(np.random.randint(100000,size=1).item())+'.jpg',im18)\n",
    "        \n",
    "        cv2.imwrite(path+'four/'+str(np.random.randint(100000,size=1).item())+'.jpg',im19)\n",
    "        cv2.imwrite(path+'four/'+str(np.random.randint(100000,size=1).item())+'.jpg',im20)\n",
    "        cv2.imwrite(path+'four/'+str(np.random.randint(100000,size=1).item())+'.jpg',im21)\n",
    "        cv2.imwrite(path+'four/'+str(np.random.randint(100000,size=1).item())+'.jpg',im22)\n",
    "        cv2.imwrite(path+'four/'+str(np.random.randint(100000,size=1).item())+'.jpg',im23)\n",
    "        cv2.imwrite(path+'four/'+str(np.random.randint(100000,size=1).item())+'.jpg',im24)\n",
    "        \n",
    "        cv2.imwrite(path+'five/'+str(np.random.randint(100000,size=1).item())+'.jpg',im25)\n",
    "        cv2.imwrite(path+'five/'+str(np.random.randint(100000,size=1).item())+'.jpg',im26)\n",
    "        cv2.imwrite(path+'five/'+str(np.random.randint(100000,size=1).item())+'.jpg',im27)\n",
    "        cv2.imwrite(path+'five/'+str(np.random.randint(100000,size=1).item())+'.jpg',im28)\n",
    "        cv2.imwrite(path+'five/'+str(np.random.randint(100000,size=1).item())+'.jpg',im29)\n",
    "        cv2.imwrite(path+'five/'+str(np.random.randint(100000,size=1).item())+'.jpg',im30)\n",
    "        \n",
    "        cv2.imwrite(path+'six/'+str(np.random.randint(100000,size=1).item())+'.jpg',im31)\n",
    "        cv2.imwrite(path+'six/'+str(np.random.randint(100000,size=1).item())+'.jpg',im32)\n",
    "        cv2.imwrite(path+'six/'+str(np.random.randint(100000,size=1).item())+'.jpg',im33)\n",
    "        cv2.imwrite(path+'six/'+str(np.random.randint(100000,size=1).item())+'.jpg',im34)\n",
    "        cv2.imwrite(path+'six/'+str(np.random.randint(100000,size=1).item())+'.jpg',im35)\n",
    "        cv2.imwrite(path+'six/'+str(np.random.randint(100000,size=1).item())+'.jpg',im36)\n",
    "        \n",
    "        cv2.imwrite(path+'seven/'+str(np.random.randint(100000,size=1).item())+'.jpg',im37)\n",
    "        cv2.imwrite(path+'seven/'+str(np.random.randint(100000,size=1).item())+'.jpg',im38)\n",
    "        cv2.imwrite(path+'seven/'+str(np.random.randint(100000,size=1).item())+'.jpg',im39)\n",
    "        cv2.imwrite(path+'seven/'+str(np.random.randint(100000,size=1).item())+'.jpg',im40)\n",
    "        cv2.imwrite(path+'seven/'+str(np.random.randint(100000,size=1).item())+'.jpg',im41)\n",
    "        cv2.imwrite(path+'seven/'+str(np.random.randint(100000,size=1).item())+'.jpg',im42)\n",
    "        \n",
    "        cv2.imwrite(path+'eight/'+str(np.random.randint(100000,size=1).item())+'.jpg',im43)\n",
    "        cv2.imwrite(path+'eight/'+str(np.random.randint(100000,size=1).item())+'.jpg',im44)\n",
    "        cv2.imwrite(path+'eight/'+str(np.random.randint(100000,size=1).item())+'.jpg',im45)\n",
    "        cv2.imwrite(path+'eight/'+str(np.random.randint(100000,size=1).item())+'.jpg',im46)\n",
    "        cv2.imwrite(path+'eight/'+str(np.random.randint(100000,size=1).item())+'.jpg',im47)\n",
    "        cv2.imwrite(path+'eight/'+str(np.random.randint(100000,size=1).item())+'.jpg',im48)\n",
    "        \n",
    "        cv2.imwrite(path+'nine/'+str(np.random.randint(100000,size=1).item())+'.jpg',im49)\n",
    "        cv2.imwrite(path+'nine/'+str(np.random.randint(100000,size=1).item())+'.jpg',im50)\n",
    "        cv2.imwrite(path+'nine/'+str(np.random.randint(100000,size=1).item())+'.jpg',im51)\n",
    "        cv2.imwrite(path+'nine/'+str(np.random.randint(100000,size=1).item())+'.jpg',im52)\n",
    "        cv2.imwrite(path+'nine/'+str(np.random.randint(100000,size=1).item())+'.jpg',im53)\n",
    "        cv2.imwrite(path+'nine/'+str(np.random.randint(100000,size=1).item())+'.jpg',im54)\n",
    "        \n",
    "        cv2.imwrite(path+'zero/'+str(np.random.randint(100000,size=1).item())+'.jpg',im55)\n",
    "        cv2.imwrite(path+'zero/'+str(np.random.randint(100000,size=1).item())+'.jpg',im56)\n",
    "        cv2.imwrite(path+'zero/'+str(np.random.randint(100000,size=1).item())+'.jpg',im57)\n",
    "        cv2.imwrite(path+'zero/'+str(np.random.randint(100000,size=1).item())+'.jpg',im58)\n",
    "        cv2.imwrite(path+'zero/'+str(np.random.randint(100000,size=1).item())+'.jpg',im59)\n",
    "        cv2.imwrite(path+'zero/'+str(np.random.randint(100000,size=1).item())+'.jpg',im60)\n",
    "        \n",
    "        \n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/zhaoh/Downloads/FYP/dataset/previous train/'#segment the training images from the ipad\n",
    "i=0\n",
    "for f in os.listdir(path):\n",
    "    name=os.path.join(path+str(f))\n",
    "    if \"png\" in name:\n",
    "        im = cv2.imread(name)\n",
    "        plt.imshow(im)\n",
    "        plt.show()\n",
    "        im1=im[300:435,420:575]#vertical 135,horizontal 155\n",
    "        im2=im[300:435,580:735]\n",
    "        im3=im[300:435,740:895]\n",
    "        im4=im[300:435,900:1055]\n",
    "        im5=im[300:435,1060:1210]\n",
    "        im6=im[300:435,1215:1370]\n",
    "        \n",
    "        im7=im[440:575,420:575]\n",
    "        im8=im[440:575,580:735]\n",
    "        im9=im[440:575,740:895]\n",
    "        im10=im[440:575,900:1055]\n",
    "        im11=im[440:575,1060:1210]\n",
    "        im12=im[440:575,1215:1370]\n",
    "        \n",
    "        im13=im[580:715,420:575]\n",
    "        im14=im[580:715,580:735]\n",
    "        im15=im[580:715,740:895]\n",
    "        im16=im[580:715,900:1055]\n",
    "        im17=im[580:715,1060:1210]\n",
    "        im18=im[580:715,1215:1370]\n",
    "        \n",
    "        im19=im[720:855,420:575]\n",
    "        im20=im[720:855,580:735]\n",
    "        im21=im[720:855,740:895]\n",
    "        im22=im[720:855,900:1055]\n",
    "        im23=im[720:855,1060:1210]\n",
    "        im24=im[720:855,1215:1370]\n",
    "        \n",
    "        im25=im[860:995,420:575]\n",
    "        im26=im[860:995,580:735]\n",
    "        im27=im[860:995,740:895]\n",
    "        im28=im[860:995,900:1055]\n",
    "        im29=im[860:995,1060:1210]\n",
    "        im30=im[860:995,1215:1370]\n",
    "        \n",
    "        im31=im[1000:1140,420:575]\n",
    "        im32=im[1000:1140,580:735]\n",
    "        im33=im[1000:1140,740:895]\n",
    "        im34=im[1000:1140,900:1055]\n",
    "        im35=im[1000:1140,1060:1210]\n",
    "        im36=im[1000:1140,1215:1370]\n",
    "        \n",
    "        im37=im[1145:1280,420:575]\n",
    "        im38=im[1145:1280,580:735]\n",
    "        im39=im[1145:1280,740:895]\n",
    "        im40=im[1145:1280,900:1055]\n",
    "        im41=im[1145:1280,1060:1210]\n",
    "        im42=im[1145:1280,1215:1370]\n",
    "        \n",
    "        im43=im[1285:1420,420:575]\n",
    "        im44=im[1285:1420,580:735]\n",
    "        im45=im[1285:1420,740:895]\n",
    "        im46=im[1285:1420,900:1055]\n",
    "        im47=im[1285:1420,1060:1210]\n",
    "        im48=im[1285:1420,1215:1370]\n",
    "        \n",
    "        im49=im[1425:1560,420:575]\n",
    "        im50=im[1425:1560,580:735]\n",
    "        im51=im[1425:1560,740:895]\n",
    "        im52=im[1425:1560,900:1055]\n",
    "        im53=im[1425:1560,1060:1210]\n",
    "        im54=im[1425:1560,1215:1370]\n",
    "        \n",
    "        #im5=im[870:990,420:575]\n",
    "        \n",
    "        plt.imshow(im54)\n",
    "        i+=1\n",
    "        #im=255-im\n",
    "        #flip = cv2.flip(im, 1)\n",
    "        \n",
    "        cv2.imwrite(path+'plus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im1)\n",
    "        cv2.imwrite(path+'plus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im7)\n",
    "        cv2.imwrite(path+'plus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im13)\n",
    "        cv2.imwrite(path+'plus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im19)\n",
    "        cv2.imwrite(path+'plus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im25)\n",
    "        cv2.imwrite(path+'plus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im31)\n",
    "        cv2.imwrite(path+'plus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im37)\n",
    "        cv2.imwrite(path+'plus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im43)\n",
    "        cv2.imwrite(path+'plus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im49)\n",
    "        \n",
    "        \n",
    "        cv2.imwrite(path+'minus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im2)\n",
    "        cv2.imwrite(path+'minus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im8)\n",
    "        cv2.imwrite(path+'minus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im14)\n",
    "        cv2.imwrite(path+'minus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im20)\n",
    "        cv2.imwrite(path+'minus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im26)\n",
    "        cv2.imwrite(path+'minus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im32)\n",
    "        cv2.imwrite(path+'minus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im38)\n",
    "        cv2.imwrite(path+'minus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im44)\n",
    "        cv2.imwrite(path+'minus/'+str(np.random.randint(10000,size=1).item())+'.jpg',im50)\n",
    "        \n",
    "        cv2.imwrite(path+'times/'+str(np.random.randint(10000,size=1).item())+'.jpg',im3)\n",
    "        cv2.imwrite(path+'times/'+str(np.random.randint(10000,size=1).item())+'.jpg',im9)\n",
    "        cv2.imwrite(path+'times/'+str(np.random.randint(10000,size=1).item())+'.jpg',im15)\n",
    "        cv2.imwrite(path+'times/'+str(np.random.randint(10000,size=1).item())+'.jpg',im21)\n",
    "        cv2.imwrite(path+'times/'+str(np.random.randint(10000,size=1).item())+'.jpg',im27)\n",
    "        cv2.imwrite(path+'times/'+str(np.random.randint(10000,size=1).item())+'.jpg',im33)\n",
    "        cv2.imwrite(path+'times/'+str(np.random.randint(10000,size=1).item())+'.jpg',im39)\n",
    "        cv2.imwrite(path+'times/'+str(np.random.randint(10000,size=1).item())+'.jpg',im45)\n",
    "        cv2.imwrite(path+'times/'+str(np.random.randint(10000,size=1).item())+'.jpg',im51)\n",
    "        \n",
    "        cv2.imwrite(path+'div/'+str(np.random.randint(10000,size=1).item())+'.jpg',im4)\n",
    "        cv2.imwrite(path+'div/'+str(np.random.randint(10000,size=1).item())+'.jpg',im10)\n",
    "        cv2.imwrite(path+'div/'+str(np.random.randint(10000,size=1).item())+'.jpg',im16)\n",
    "        cv2.imwrite(path+'div/'+str(np.random.randint(10000,size=1).item())+'.jpg',im22)\n",
    "        cv2.imwrite(path+'div/'+str(np.random.randint(10000,size=1).item())+'.jpg',im28)\n",
    "        cv2.imwrite(path+'div/'+str(np.random.randint(10000,size=1).item())+'.jpg',im34)\n",
    "        cv2.imwrite(path+'div/'+str(np.random.randint(10000,size=1).item())+'.jpg',im40)\n",
    "        cv2.imwrite(path+'div/'+str(np.random.randint(10000,size=1).item())+'.jpg',im46)\n",
    "        cv2.imwrite(path+'div/'+str(np.random.randint(10000,size=1).item())+'.jpg',im52)\n",
    "        \n",
    "        cv2.imwrite(path+'equal/'+str(np.random.randint(10000,size=1).item())+'.jpg',im5)\n",
    "        cv2.imwrite(path+'equal/'+str(np.random.randint(10000,size=1).item())+'.jpg',im11)\n",
    "        cv2.imwrite(path+'equal/'+str(np.random.randint(10000,size=1).item())+'.jpg',im17)\n",
    "        cv2.imwrite(path+'equal/'+str(np.random.randint(10000,size=1).item())+'.jpg',im23)\n",
    "        cv2.imwrite(path+'equal/'+str(np.random.randint(10000,size=1).item())+'.jpg',im29)\n",
    "        cv2.imwrite(path+'equal/'+str(np.random.randint(10000,size=1).item())+'.jpg',im35)\n",
    "        cv2.imwrite(path+'equal/'+str(np.random.randint(10000,size=1).item())+'.jpg',im41)\n",
    "        cv2.imwrite(path+'equal/'+str(np.random.randint(10000,size=1).item())+'.jpg',im47)\n",
    "        cv2.imwrite(path+'equal/'+str(np.random.randint(10000,size=1).item())+'.jpg',im53)\n",
    "        '''\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im6)\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im12)\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im18)\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im24)\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im30)\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im36)\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im42)\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im48)\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im54)\n",
    "        '''\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invert black and white of MNIST, only do it once\n",
    "'''\n",
    "path='C:/Users/zhaoh/Downloads/FYP/dataset/Mnist/zero/'\n",
    "i=0\n",
    "for f in os.listdir(path):\n",
    "    name=os.path.join(path+str(f))\n",
    "    if \"jpg\" in name:\n",
    "        im = cv2.imread(name)\n",
    "        i+=1\n",
    "        im=255-im#invert the color \n",
    "        \n",
    "        cv2.imwrite(name,im)\n",
    "print(i)#number of images\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the dataloader object which loads the training images and gives them a label \n",
    "class DrivingDataset(Dataset):\n",
    "    def __init__(self,data_dir, input_w=224, input_h=224,is_train=True,transform=None):\n",
    "        if is_train==False:\n",
    "            threshold=55#use 50 from each class as validation\n",
    "        else:\n",
    "            threshold=480#300 as training\n",
    "        namelist = [0 for i in range(16)]# 15 list to contain images for each class\n",
    "        \n",
    "        self.data_filenames = []\n",
    "        self.data_ids = []\n",
    "        self.is_train=is_train\n",
    "\n",
    "        self.data_root=fs.open_fs(data_dir)\n",
    "        self.transform = transform\n",
    "        keyword=['zero','one','two','three','four','five','six','seven','eight','nine','plus','minus','times','div','equal','decimal']\n",
    "        for p in self.data_root.walk.files(filter=[\"*.jpg\",\"*.png\"]):\n",
    "            filename=data_dir+p\n",
    "            if is_train==True or 1==1:#temporary bug if in training, a label will be given to a training image depending on its folder name\n",
    "                #like all images of 4 is contained in the folder \"four\"\n",
    "                for i,j in enumerate(keyword):\n",
    "                    if j in filename:\n",
    "                        if namelist[i]<threshold:\n",
    "                            self.data_filenames.append(filename)\n",
    "                            self.data_ids.append(i)\n",
    "                            namelist[i]+=1\n",
    "                \n",
    "            else:#if not training, it is not necessary to load a label\n",
    "                self.data_filenames.append(filename)\n",
    "                #self.data_ids.append(0)\n",
    "        \n",
    "        \n",
    "        # print(self.data_filenames)\n",
    "        #print(namelist)\n",
    "        print(len(self.data_filenames))#displays how many images are there in a class\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Grey(i, j) = 0.299 × R(i, j) + 0.587 × G(i, j) + 0.114 × B(i, j)\"\"\"\n",
    "\n",
    "        img_path = self.data_filenames[item]\n",
    "        #print(img_path)\n",
    "        target = self.data_ids[item]\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        target = np.array([target], dtype=np.long)\n",
    "        target = torch.from_numpy(target)\n",
    "        \n",
    "        return image,target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_filenames)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#新的transform，通过padding保证不畸变图片,ratio是需要设置的最终图片长宽比\n",
    "class ratio_crop(object):\n",
    "    def __init__(self, ratio=1.0):\n",
    "        self.ratio = ratio\n",
    "    def __call__(self, images):\n",
    "            ratio=1.0\n",
    "            #for img in images:\n",
    "            #print(images.shape)\n",
    "            w=images.shape[1]\n",
    "            h=images.shape[0]\n",
    "            aspect_ratio=float(w)/float(h)\n",
    "            #print(images.shape,aspect_ratio)\n",
    "            if aspect_ratio==ratio:\n",
    "                a=1\n",
    "            elif aspect_ratio>ratio:\n",
    "                dif = np.abs(w  - h)\n",
    "                pad1, pad2 = int(dif // 2), int(dif - dif // 2)\n",
    "                pad = ((0, 0),(pad1, pad2) ,(0, 0))\n",
    "                images = np.pad(images, pad, \"constant\", constant_values=255)\n",
    "                #input_img = cv2.resize(input_x, (inputwidth, inputheight))\n",
    "            else:\n",
    "                # padding w\n",
    "                dif = np.abs(h  - w)\n",
    "                pad1, pad2 = int(dif // 2), int(dif - dif // 2)\n",
    "                pad = ((0, 0),(pad1, pad2),(0, 0))\n",
    "                images = np.pad(images, pad, \"constant\", constant_values=255)\n",
    "                #input_img = cv2.resize(input_x, (inputwidth, inputheight))\n",
    "            return images\n",
    "        \n",
    "transform = transforms.Compose([\n",
    "            ratio_crop(1.0),\n",
    "            transforms.ToPILImage(),\n",
    "            #transforms.Resize((28,28), interpolation=2),\n",
    "            #transforms.Pad(5, fill=255, padding_mode='constant'),\n",
    "            #transforms.RandomResizedCrop(56, scale=(0.9, 1.0)),\n",
    "            transforms.Resize((56,56), interpolation=2),\n",
    "            transforms.ToTensor(),\n",
    "    ]) \n",
    "transform2 = transforms.Compose([\n",
    "            ratio_crop(1.0),\n",
    "            transforms.ToPILImage(),\n",
    "            #transforms.Resize((28,28), interpolation=2),\n",
    "            #transforms.Pad(5, fill=255, padding_mode='constant'),\n",
    "            #transforms.RandomResizedCrop(56, scale=(0.7, 1.0)),\n",
    "            transforms.Resize((56,56), interpolation=2),\n",
    "            transforms.ToTensor(),\n",
    "    ]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7177\n",
      "868\n",
      "408\n"
     ]
    }
   ],
   "source": [
    "train_set = DrivingDataset(data_dir=\"C:/Users/zhaoh/Downloads/FYP/dataset/train/\", is_train=True,transform=transform) \n",
    "val_set = DrivingDataset(data_dir=\"C:/Users/zhaoh/Downloads/FYP/dataset/eval/\", is_train=False,transform=transform) \n",
    "test_set = DrivingDataset(data_dir=\"C:/Users/zhaoh/Downloads/FYP/dataset/test/\", is_train=False,transform=transform2) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "valloader = torch.utils.data.DataLoader(val_set, batch_size=10,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=10,\n",
    "                                          shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for dimension 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-e36b959cef39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gray\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 10 is out of bounds for dimension 0 with size 10"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#take a look of training set\n",
    "image_batch,label= next(iter(trainloader))\n",
    "for i in range(20): \n",
    "    img = (image_batch[i].numpy())\n",
    "    plt.subplot(2,4,(i%8+1))\n",
    "    plt.imshow(img[0],cmap = \"gray\")\n",
    "    if i%8==0:\n",
    "        plt.show()\n",
    "    \n",
    "print(img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the network below\n",
    "####################################\n",
    "#######################################\n",
    "###########################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.hub import load_state_dict_from_url \n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
    "           'wide_resnet50_2', 'wide_resnet101_2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
    "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    __constants__ = ['downsample']\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    __constants__ = ['downsample']\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)###########################3 input channels\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # Allow for accessing forward method in a inherited class\n",
    "    forward = _forward\n",
    "\n",
    "\n",
    "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def resnet152(pretrained=True, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-152 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "def resnet18(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         # input shape (1, 28, 28)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              # input height\n",
    "                out_channels=16,            # n_filters\n",
    "                kernel_size=5,              # filter size\n",
    "                stride=1,                   # filter movement/step\n",
    "                padding=2,                  # if want same width and length of this image after con2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # output shape (16, 28, 28)\n",
    "            nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(16),# activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area, output shape (16, 14, 14)\n",
    "        )#32\n",
    "        self.conv2 = nn.Sequential(         # input shape (16, 14, 14)\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     # output shape (32, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(32),# activation\n",
    "            nn.MaxPool2d(2),                # output shape (32, 7, 7)#16\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(         # input shape (16, 14, 14)\n",
    "            nn.Conv2d(32, 32, 5, 1, 2),     # output shape (32, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(32),# activation\n",
    "            nn.MaxPool2d(2),                # output shape (32, 7, 7)#8\n",
    "        )\n",
    "        self.out1 = nn.Sequential(nn.Linear(32 * 7 * 7, 512),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.out2 = nn.Linear(512, 16)\n",
    "        # fully connected layer, output 10 classes\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)   \n",
    "        x=self.out1(x)\n",
    "        #print(x.shape)\n",
    "        ## flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        output = self.out2(x)\n",
    "        return output   # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (out1): Sequential(\n",
      "    (0): Linear(in_features=1152, out_features=16, bias=True)\n",
      "    (1): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "\n",
    "fc_features = model.fc.in_features\n",
    "#修改类别为9\n",
    "model.fc = nn.Linear(fc_features, 16)\n",
    "features = model.conv1.in_channels\n",
    "\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)#change channels\n",
    "#model.load_state_dict(torch.load('1channel20'))#########################\n",
    "model=torch.load('1channel88.pkl')\n",
    "#print(model)\n",
    "#model.to(device)\n",
    "\n",
    "model =visual()\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion=nn.MultiMarginLoss()\n",
    "learning_rate = 1e-4\n",
    "#print(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_predictions = 0.0\n",
    "    correct_predictions = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for label,[data, target] in tqdm(enumerate(train_loader)):   \n",
    "        optimizer.zero_grad()\n",
    "        data = data[:,0]\n",
    "        data=data.reshape(-1,1,56,56).float().to(device)#data = data.float().to(device)\n",
    "        #print(target)\n",
    "        target=target.squeeze(1)\n",
    "        target = target.long().to(device)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        #loss+=criterion_hinge(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += target.size(0)\n",
    "        correct_predictions += (predicted == target).sum().item()\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    running_loss /= len(train_loader)\n",
    "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time,' train accuracy',acc)\n",
    "    return running_loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "        #Predicted=[]\n",
    "        for data, target in (test_loader):\n",
    "            data = data[:,0]\n",
    "            data=data.reshape(-1,1,56,56).float().to(device)#data = data.float().to(device)\n",
    "            #print(data.shape)\n",
    "            target=target.squeeze(1)\n",
    "            target = target.long().to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            #print(predicted.shape)\n",
    "            #Predicted.append(predicted)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, target).detach()\n",
    "            #loss+=criterion_hinge(outputs, target).detach()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Testing Loss: ', running_loss)\n",
    "        print('Testing Accuracy: ', acc, '%')\n",
    "        return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "4it [00:00, 38.94it/s]\n",
      "9it [00:00, 41.22it/s]\n",
      "14it [00:00, 42.65it/s]\n",
      "20it [00:00, 44.95it/s]\n",
      "26it [00:00, 46.28it/s]\n",
      "32it [00:00, 47.94it/s]\n",
      "38it [00:00, 49.30it/s]\n",
      "43it [00:00, 48.53it/s]\n",
      "48it [00:00, 48.30it/s]\n",
      "53it [00:01, 48.69it/s]\n",
      "59it [00:01, 50.47it/s]\n",
      "65it [00:01, 50.37it/s]\n",
      "71it [00:01, 51.33it/s]\n",
      "77it [00:01, 51.36it/s]\n",
      "83it [00:01, 51.11it/s]\n",
      "89it [00:01, 51.87it/s]\n",
      "95it [00:01, 52.55it/s]\n",
      "101it [00:02, 50.62it/s]\n",
      "107it [00:02, 50.47it/s]\n",
      "113it [00:02, 51.14it/s]\n",
      "119it [00:02, 51.62it/s]\n",
      "125it [00:02, 49.77it/s]\n",
      "131it [00:02, 50.25it/s]\n",
      "137it [00:02, 48.28it/s]\n",
      "143it [00:02, 49.30it/s]\n",
      "148it [00:02, 49.11it/s]\n",
      "154it [00:03, 50.53it/s]\n",
      "160it [00:03, 50.16it/s]\n",
      "166it [00:03, 50.40it/s]\n",
      "172it [00:03, 49.95it/s]\n",
      "178it [00:03, 49.63it/s]\n",
      "183it [00:03, 48.62it/s]\n",
      "192it [00:03, 49.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  0.6359942623142464 Time:  3.871668815612793  train accuracy 74.32291666666667\n",
      "Testing Loss:  1.2989351482226932\n",
      "Testing Accuracy:  64.97695852534562 %\n",
      "Testing Loss:  1.081568732494261\n",
      "Testing Accuracy:  69.6078431372549 %\n",
      "0 epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "5it [00:00, 45.58it/s]\n",
      "10it [00:00, 46.08it/s]\n",
      "16it [00:00, 47.90it/s]\n",
      "22it [00:00, 48.78it/s]\n",
      "27it [00:00, 48.89it/s]\n",
      "32it [00:00, 48.27it/s]\n",
      "37it [00:00, 47.69it/s]\n",
      "42it [00:00, 47.99it/s]\n",
      "47it [00:00, 47.91it/s]\n",
      "53it [00:01, 49.27it/s]\n",
      "59it [00:01, 49.77it/s]\n",
      "65it [00:01, 49.88it/s]\n",
      "71it [00:01, 50.72it/s]\n",
      "77it [00:01, 49.05it/s]\n",
      "83it [00:01, 49.99it/s]\n",
      "88it [00:01, 49.29it/s]\n",
      "94it [00:01, 50.17it/s]\n",
      "100it [00:02, 48.57it/s]\n",
      "106it [00:02, 49.39it/s]\n",
      "111it [00:02, 49.32it/s]\n",
      "116it [00:02, 49.27it/s]\n",
      "121it [00:02, 48.80it/s]\n",
      "126it [00:02, 48.76it/s]\n",
      "132it [00:02, 49.65it/s]\n",
      "138it [00:02, 51.19it/s]\n",
      "144it [00:02, 50.62it/s]\n",
      "150it [00:03, 49.97it/s]\n",
      "156it [00:03, 49.65it/s]\n",
      "161it [00:03, 49.20it/s]\n",
      "166it [00:03, 48.11it/s]\n",
      "172it [00:03, 48.94it/s]\n",
      "177it [00:03, 48.86it/s]\n",
      "182it [00:03, 48.38it/s]\n",
      "192it [00:03, 49.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  0.6314186036470346 Time:  3.892199754714966  train accuracy 74.73958333333334\n",
      "Testing Loss:  1.2961294277646076\n",
      "Testing Accuracy:  65.09216589861751 %\n",
      "Testing Loss:  1.0527280525463383\n",
      "Testing Accuracy:  69.6078431372549 %\n",
      "1 epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "5it [00:00, 49.15it/s]\n",
      "11it [00:00, 49.93it/s]\n",
      "16it [00:00, 49.55it/s]\n",
      "21it [00:00, 48.42it/s]\n",
      "26it [00:00, 48.78it/s]\n",
      "31it [00:00, 48.33it/s]\n",
      "36it [00:00, 47.47it/s]\n",
      "41it [00:00, 48.10it/s]\n",
      "46it [00:00, 47.99it/s]\n",
      "52it [00:01, 48.61it/s]\n",
      "58it [00:01, 49.42it/s]\n",
      "63it [00:01, 49.20it/s]\n",
      "68it [00:01, 49.04it/s]\n",
      "74it [00:01, 50.48it/s]\n",
      "80it [00:01, 51.02it/s]\n",
      "86it [00:01, 50.75it/s]\n",
      "92it [00:01, 50.95it/s]\n",
      "98it [00:01, 50.70it/s]\n",
      "104it [00:02, 51.83it/s]\n",
      "110it [00:02, 52.66it/s]\n",
      "116it [00:02, 51.34it/s]\n",
      "122it [00:02, 51.23it/s]\n",
      "128it [00:02, 51.16it/s]\n",
      "134it [00:02, 50.46it/s]\n",
      "140it [00:02, 50.49it/s]\n",
      "146it [00:02, 50.00it/s]\n",
      "152it [00:03, 50.42it/s]\n",
      "158it [00:03, 50.08it/s]\n",
      "164it [00:03, 50.73it/s]\n",
      "170it [00:03, 51.72it/s]\n",
      "176it [00:03, 51.11it/s]\n",
      "182it [00:03, 52.00it/s]\n",
      "192it [00:03, 50.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  0.62314598976324 Time:  3.821810722351074  train accuracy 74.21875\n",
      "Testing Loss:  1.3248309136807233\n",
      "Testing Accuracy:  64.40092165898618 %\n",
      "Testing Loss:  1.0658266239049958\n",
      "Testing Accuracy:  67.64705882352942 %\n",
      "2 epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "6it [00:00, 50.97it/s]\n",
      "12it [00:00, 50.72it/s]\n",
      "17it [00:00, 50.09it/s]\n",
      "22it [00:00, 48.64it/s]\n",
      "27it [00:00, 46.61it/s]\n",
      "32it [00:00, 46.30it/s]\n",
      "38it [00:00, 48.07it/s]\n",
      "43it [00:00, 48.25it/s]\n",
      "48it [00:00, 48.10it/s]\n",
      "54it [00:01, 49.05it/s]\n",
      "59it [00:01, 45.98it/s]\n",
      "64it [00:01, 46.75it/s]\n",
      "70it [00:01, 48.06it/s]\n",
      "75it [00:01, 48.39it/s]\n",
      "81it [00:01, 49.87it/s]\n",
      "87it [00:01, 50.83it/s]\n",
      "93it [00:01, 50.49it/s]\n",
      "99it [00:02, 50.51it/s]\n",
      "105it [00:02, 49.16it/s]\n",
      "111it [00:02, 50.32it/s]\n",
      "117it [00:02, 50.64it/s]\n",
      "123it [00:02, 49.86it/s]\n",
      "129it [00:02, 50.07it/s]\n",
      "135it [00:02, 48.87it/s]\n",
      "140it [00:02, 46.13it/s]\n",
      "145it [00:02, 45.10it/s]\n",
      "150it [00:03, 44.28it/s]\n",
      "155it [00:03, 43.96it/s]\n",
      "160it [00:03, 44.31it/s]\n",
      "165it [00:03, 44.10it/s]\n",
      "170it [00:03, 43.83it/s]\n",
      "175it [00:03, 44.22it/s]\n",
      "180it [00:03, 44.38it/s]\n",
      "185it [00:03, 45.46it/s]\n",
      "192it [00:04, 47.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  0.6005029326770455 Time:  4.03903603553772  train accuracy 75.625\n",
      "Testing Loss:  1.3019739947784905\n",
      "Testing Accuracy:  64.86175115207374 %\n",
      "Testing Loss:  1.0230002105236053\n",
      "Testing Accuracy:  68.62745098039215 %\n",
      "3 epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "5it [00:00, 48.68it/s]\n",
      "10it [00:00, 48.96it/s]\n",
      "16it [00:00, 50.04it/s]\n",
      "22it [00:00, 51.75it/s]\n",
      "28it [00:00, 52.19it/s]\n",
      "33it [00:00, 50.62it/s]\n",
      "38it [00:00, 48.28it/s]\n",
      "44it [00:00, 48.94it/s]\n",
      "49it [00:00, 49.15it/s]\n",
      "54it [00:01, 48.86it/s]\n",
      "60it [00:01, 49.73it/s]\n",
      "65it [00:01, 49.55it/s]\n",
      "70it [00:01, 49.28it/s]\n",
      "75it [00:01, 49.25it/s]\n",
      "80it [00:01, 49.07it/s]\n",
      "86it [00:01, 50.00it/s]\n",
      "92it [00:01, 50.29it/s]\n",
      "98it [00:01, 50.12it/s]\n",
      "104it [00:02, 50.50it/s]\n",
      "110it [00:02, 50.39it/s]\n",
      "116it [00:02, 49.75it/s]\n",
      "122it [00:02, 51.92it/s]\n",
      "128it [00:02, 50.72it/s]\n",
      "134it [00:02, 50.42it/s]\n",
      "140it [00:02, 50.84it/s]\n",
      "146it [00:02, 51.01it/s]\n",
      "152it [00:03, 51.40it/s]\n",
      "158it [00:03, 52.07it/s]\n",
      "164it [00:03, 51.47it/s]\n",
      "170it [00:03, 51.86it/s]\n",
      "176it [00:03, 51.99it/s]\n",
      "182it [00:03, 52.09it/s]\n",
      "192it [00:03, 50.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  0.5464221159636509 Time:  3.8004024028778076  train accuracy 76.40625\n",
      "Testing Loss:  1.2978097652566845\n",
      "Testing Accuracy:  65.09216589861751 %\n",
      "Testing Loss:  1.0467578448900363\n",
      "Testing Accuracy:  69.11764705882352 %\n",
      "4 epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "6it [00:00, 51.86it/s]\n",
      "12it [00:00, 51.73it/s]\n",
      "17it [00:00, 50.62it/s]\n",
      "22it [00:00, 48.99it/s]\n",
      "28it [00:00, 49.69it/s]\n",
      "34it [00:00, 50.58it/s]\n",
      "39it [00:00, 49.69it/s]\n",
      "44it [00:00, 49.53it/s]\n",
      "49it [00:00, 49.56it/s]\n",
      "54it [00:01, 49.00it/s]\n",
      "59it [00:01, 48.90it/s]\n",
      "65it [00:01, 49.51it/s]\n",
      "71it [00:01, 51.75it/s]\n",
      "77it [00:01, 51.12it/s]\n",
      "83it [00:01, 51.61it/s]\n",
      "89it [00:01, 52.36it/s]\n",
      "95it [00:01, 49.50it/s]\n",
      "100it [00:02, 48.82it/s]\n",
      "105it [00:02, 48.63it/s]\n",
      "111it [00:02, 50.18it/s]\n",
      "117it [00:02, 50.93it/s]\n",
      "123it [00:02, 50.18it/s]\n",
      "129it [00:02, 49.06it/s]\n",
      "134it [00:02, 47.82it/s]\n",
      "139it [00:02, 48.21it/s]\n",
      "145it [00:02, 49.25it/s]\n",
      "151it [00:03, 50.64it/s]\n",
      "157it [00:03, 51.26it/s]\n",
      "163it [00:03, 51.44it/s]\n",
      "169it [00:03, 50.79it/s]\n",
      "175it [00:03, 50.59it/s]\n",
      "181it [00:03, 52.02it/s]\n",
      "192it [00:03, 50.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  0.5586866745725274 Time:  3.819812297821045  train accuracy 75.78125\n",
      "Testing Loss:  1.3098855371447815\n",
      "Testing Accuracy:  65.78341013824884 %\n",
      "Testing Loss:  1.0126438849582904\n",
      "Testing Accuracy:  70.58823529411765 %\n",
      "5 epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "5it [00:00, 48.67it/s]\n",
      "11it [00:00, 49.71it/s]\n",
      "16it [00:00, 49.10it/s]\n",
      "22it [00:00, 50.40it/s]\n",
      "28it [00:00, 50.58it/s]\n",
      "33it [00:00, 48.82it/s]\n",
      "38it [00:00, 49.07it/s]\n",
      "44it [00:00, 50.75it/s]\n",
      "50it [00:00, 51.61it/s]\n",
      "56it [00:01, 51.29it/s]\n",
      "62it [00:01, 50.30it/s]\n",
      "68it [00:01, 50.00it/s]\n",
      "73it [00:01, 49.74it/s]\n",
      "78it [00:01, 49.42it/s]\n",
      "84it [00:01, 50.50it/s]\n",
      "90it [00:01, 50.39it/s]\n",
      "96it [00:01, 50.44it/s]\n",
      "102it [00:02, 49.85it/s]\n",
      "108it [00:02, 50.06it/s]\n",
      "114it [00:02, 50.84it/s]\n",
      "120it [00:02, 50.63it/s]\n",
      "126it [00:02, 51.26it/s]\n",
      "132it [00:02, 50.78it/s]\n",
      "138it [00:02, 50.08it/s]\n",
      "144it [00:02, 50.47it/s]\n",
      "150it [00:02, 50.25it/s]\n",
      "156it [00:03, 50.21it/s]\n",
      "162it [00:03, 50.57it/s]\n",
      "168it [00:03, 51.74it/s]\n",
      "174it [00:03, 50.47it/s]\n",
      "180it [00:03, 51.80it/s]\n",
      "186it [00:03, 51.29it/s]\n",
      "192it [00:03, 50.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  0.561708478324969 Time:  3.7958462238311768  train accuracy 75.20833333333333\n",
      "Testing Loss:  1.2997217558581253\n",
      "Testing Accuracy:  66.3594470046083 %\n",
      "Testing Loss:  1.005237258789016\n",
      "Testing Accuracy:  70.58823529411765 %\n",
      "6 epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "5it [00:00, 45.57it/s]\n",
      "11it [00:00, 47.40it/s]\n",
      "17it [00:00, 48.66it/s]\n",
      "23it [00:00, 49.58it/s]\n",
      "28it [00:00, 47.48it/s]\n",
      "33it [00:00, 47.97it/s]\n",
      "38it [00:00, 46.44it/s]\n",
      "43it [00:00, 46.43it/s]\n",
      "48it [00:01, 46.17it/s]\n",
      "54it [00:01, 48.55it/s]\n",
      "59it [00:01, 48.59it/s]\n",
      "65it [00:01, 49.41it/s]\n",
      "71it [00:01, 51.01it/s]\n",
      "77it [00:01, 51.79it/s]\n",
      "83it [00:01, 50.64it/s]\n",
      "89it [00:01, 51.74it/s]\n",
      "95it [00:01, 50.99it/s]\n",
      "101it [00:02, 50.60it/s]\n",
      "107it [00:02, 50.71it/s]\n",
      "113it [00:02, 51.32it/s]\n",
      "119it [00:02, 50.82it/s]\n",
      "125it [00:02, 50.62it/s]\n",
      "131it [00:02, 48.99it/s]\n",
      "136it [00:02, 48.33it/s]\n",
      "141it [00:02, 48.57it/s]\n",
      "147it [00:02, 49.39it/s]\n",
      "152it [00:03, 48.89it/s]\n",
      "158it [00:03, 50.75it/s]\n",
      "164it [00:03, 51.08it/s]\n",
      "170it [00:03, 50.54it/s]\n",
      "176it [00:03, 50.29it/s]\n",
      "182it [00:03, 49.14it/s]\n",
      "192it [00:03, 49.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  0.5783538220809229 Time:  3.861025094985962  train accuracy 74.21875\n",
      "Testing Loss:  1.2845548667099285\n",
      "Testing Accuracy:  66.82027649769586 %\n",
      "Testing Loss:  1.0073182677350394\n",
      "Testing Accuracy:  70.58823529411765 %\n",
      "7 epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "6it [00:00, 50.54it/s]\n",
      "12it [00:00, 51.07it/s]\n",
      "17it [00:00, 49.14it/s]\n",
      "23it [00:00, 49.68it/s]\n",
      "28it [00:00, 49.37it/s]\n",
      "33it [00:00, 48.59it/s]\n",
      "38it [00:00, 47.78it/s]\n",
      "44it [00:00, 49.42it/s]\n",
      "49it [00:00, 49.48it/s]\n",
      "54it [00:01, 49.24it/s]\n",
      "59it [00:01, 48.36it/s]\n",
      "64it [00:01, 47.76it/s]\n",
      "69it [00:01, 48.03it/s]\n",
      "75it [00:01, 49.85it/s]\n",
      "81it [00:01, 50.31it/s]\n",
      "87it [00:01, 51.68it/s]\n",
      "93it [00:01, 51.87it/s]\n",
      "99it [00:01, 49.56it/s]\n",
      "105it [00:02, 50.87it/s]\n",
      "111it [00:02, 50.77it/s]\n",
      "117it [00:02, 51.36it/s]\n",
      "123it [00:02, 49.48it/s]\n",
      "128it [00:02, 46.62it/s]\n",
      "134it [00:02, 48.19it/s]\n",
      "139it [00:02, 48.34it/s]\n",
      "145it [00:02, 49.22it/s]\n",
      "150it [00:03, 49.06it/s]\n",
      "155it [00:03, 48.94it/s]\n",
      "160it [00:03, 48.30it/s]\n",
      "165it [00:03, 48.27it/s]\n",
      "171it [00:03, 49.29it/s]\n",
      "177it [00:03, 49.66it/s]\n",
      "182it [00:03, 48.93it/s]\n",
      "192it [00:03, 49.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  0.5711279232248975 Time:  3.8816182613372803  train accuracy 74.32291666666667\n",
      "Testing Loss:  1.281365435013826\n",
      "Testing Accuracy:  66.93548387096774 %\n",
      "Testing Loss:  0.9914660410183233\n",
      "Testing Accuracy:  69.85294117647058 %\n",
      "8 epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "6it [00:00, 48.90it/s]\n",
      "11it [00:00, 48.55it/s]\n",
      "16it [00:00, 47.89it/s]\n",
      "22it [00:00, 47.85it/s]\n",
      "27it [00:00, 47.82it/s]\n",
      "33it [00:00, 48.72it/s]\n",
      "39it [00:00, 49.87it/s]\n",
      "44it [00:00, 48.78it/s]\n",
      "50it [00:01, 50.80it/s]\n",
      "55it [00:01, 49.69it/s]\n",
      "60it [00:01, 48.66it/s]\n",
      "65it [00:01, 48.67it/s]\n",
      "71it [00:01, 49.10it/s]\n",
      "76it [00:01, 49.26it/s]\n",
      "81it [00:01, 49.08it/s]\n",
      "87it [00:01, 50.51it/s]\n",
      "93it [00:01, 51.04it/s]\n",
      "99it [00:01, 51.95it/s]\n",
      "105it [00:02, 51.13it/s]\n",
      "111it [00:02, 50.83it/s]\n",
      "117it [00:02, 50.87it/s]\n",
      "123it [00:02, 51.17it/s]\n",
      "129it [00:02, 51.77it/s]\n",
      "135it [00:02, 51.93it/s]\n",
      "141it [00:02, 50.47it/s]\n",
      "147it [00:02, 51.15it/s]\n",
      "153it [00:03, 49.70it/s]\n",
      "159it [00:03, 50.71it/s]\n",
      "165it [00:03, 51.05it/s]\n",
      "171it [00:03, 51.56it/s]\n",
      "177it [00:03, 49.60it/s]\n",
      "182it [00:03, 49.61it/s]\n",
      "192it [00:03, 50.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  0.5495645285069864 Time:  3.83870530128479  train accuracy 75.83333333333333\n",
      "Testing Loss:  1.325187624699768\n",
      "Testing Accuracy:  67.2811059907834 %\n",
      "Testing Loss:  1.006586467347494\n",
      "Testing Accuracy:  70.09803921568627 %\n",
      "9 epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=epoch=\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "Train_loss = []\n",
    "Test_loss = []\n",
    "Test_acc = []\n",
    "Val_loss = []\n",
    "Val_acc = []\n",
    "Train_acc=[]\n",
    "for i in range(n_epochs):\n",
    "    train_loss,train_acc = train_epoch(model, trainloader, criterion, optimizer)\n",
    "    val_loss, val_acc = test_model(model, valloader, criterion)\n",
    "    test_loss, test_acc = test_model(model, testloader, criterion)\n",
    "    Train_acc.append(train_acc)\n",
    "    Train_loss.append(train_loss)\n",
    "    Val_acc.append(val_acc)\n",
    "    Val_loss.append(val_loss)\n",
    "    Test_loss.append(test_loss)\n",
    "    Test_acc.append(test_acc)\n",
    "    print(i,'epoch='*20)\n",
    "    torch.save(model, 'modelvisual'+str(i)+'.pkl')\n",
    "#test_loss, test_acc = test_model(model, testloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Training, validation loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(Val_loss,label='validation loss')\n",
    "plt.plot(Train_loss,label='train loss')\n",
    "plt.plot(Test_loss,label='test loss')\n",
    "plt.plot()\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Training, validation accuracy')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(Val_acc,label='validation accuracy')\n",
    "plt.plot(Train_acc,label='train accuracy')\n",
    "plt.plot(Test_acc,label='test accuracy')\n",
    "plt.plot()\n",
    "plt.legend(loc='lower right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread(\"save.jpg\") \n",
    "#im=PIL.ImageOps.invert(im)\n",
    "#image = transforms.ToTensor()(im)  \n",
    "plt.imshow(255-im, cmap = 'gray')\n",
    "#image=image[0]\n",
    "# This method will show image in any image viewer  \n",
    "\n",
    "image=255-im\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image shape: torch.Size([576, 1021])\n",
    "cell0=image[200:400,0:100,:]\n",
    "cell1=image[180:400,150:250,:]\n",
    "cell2=image[180:400,300:400,:]\n",
    "cell3=image[180:400,400:550,:]\n",
    "cell4=image[200:400,550:700,:]\n",
    "cell5=image[180:400,700:800,:]\n",
    "cell6=image[130:400,800:900,:]\n",
    "cell7=image[180:400,400:550,:]\n",
    "cell8=image[100:400,100:400,:]\n",
    "\n",
    "'''\n",
    "cell0=transforms.ToPILImage()(cell0)\n",
    "cell1=transforms.ToPILImage()(cell1)\n",
    "cell2=transforms.ToPILImage()(cell2)\n",
    "cell3=transforms.ToPILImage()(cell3)\n",
    "cell4=transforms.ToPILImage()(cell4)\n",
    "cell5=transforms.ToPILImage()(cell5)\n",
    "cell6=transforms.ToPILImage()(cell6)\n",
    "cell7=transforms.ToPILImage()(cell7)\n",
    "'''\n",
    "#print(cell8.shape)\n",
    "'''\n",
    "cell0=ratio_crop(1.0)(cell0)\n",
    "cell1=ratio_crop(1.0)(cell1)\n",
    "cell2=ratio_crop(1.0)(cell2)\n",
    "cell3=ratio_crop(1.0)(cell3)\n",
    "cell4=ratio_crop(1.0)(cell4)\n",
    "cell5=ratio_crop(1.0)(cell5)\n",
    "cell6=ratio_crop(1.0)(cell6)\n",
    "cell7=ratio_crop(1.0)(cell7)\n",
    "\n",
    "print(cell0.dtype)\n",
    "'''\n",
    "'''\n",
    "cell0=cv2.resize(cell0, (56, 56))\n",
    "cell1=cv2.resize(cell1, (56, 56))\n",
    "cell2=cv2.resize(cell2, (56, 56))\n",
    "cell3=cv2.resize(cell3, (56, 56))\n",
    "cell4=cv2.resize(cell4, (56, 56))\n",
    "cell5=cv2.resize(cell5, (56, 56))\n",
    "cell6=cv2.resize(cell6, (56, 56))\n",
    "cell7=cv2.resize(cell7, (56, 56))\n",
    "'''\n",
    "#cell0=transforms.Pad(10, fill=255, padding_mode='constant')(cell0)\n",
    "#plt.subplot(4,4,1)\n",
    "#img = np.moveaxis(cell0.numpy(),0,2)\n",
    "#cell0=cell0.reshape(56,56,3)\n",
    "plt.imshow(cell0)\n",
    "#plt.savefig(\"C:/Users/zhaoh/Downloads/FYP/deeplearning new/new-number-identification/crop/\"+'1.jpg')\n",
    "plt.show()\n",
    "\n",
    "#plt.subplot(4,4,2)\n",
    "plt.imshow(cell1)\n",
    "#plt.savefig(\"C:/Users/zhaoh/Downloads/FYP/deeplearning new/new-number-identification/crop/\"+'2.jpg')\n",
    "plt.show()\n",
    "\n",
    "#plt.subplot(4,4,3)\n",
    "plt.imshow(cell2)\n",
    "#plt.savefig(\"C:/Users/zhaoh/Downloads/FYP/deeplearning new/new-number-identification/crop/\"+'3.jpg')\n",
    "plt.show()\n",
    "#plt.subplot(4,4,4)\n",
    "plt.imshow(cell3)\n",
    "plt.show()\n",
    "#plt.subplot(4,4,1)\n",
    "plt.imshow(cell4)\n",
    "plt.show()\n",
    "#plt.subplot(4,4,2)\n",
    "plt.imshow(cell5)\n",
    "plt.show()\n",
    "#plt.subplot(4,4,3)\n",
    "plt.imshow(cell6)\n",
    "plt.show()\n",
    "#plt.subplot(4,4,4)\n",
    "plt.imshow(cell7)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(cell3.shape)\n",
    "cv2.imwrite(\"C:/Users/zhaoh/Downloads/FYP/deeplearning new/new-number-identification/crop/\"+'1.jpg',cell0)\n",
    "cv2.imwrite(\"C:/Users/zhaoh/Downloads/FYP/deeplearning new/new-number-identification/crop/\"+'2.jpg',cell1)\n",
    "cv2.imwrite(\"C:/Users/zhaoh/Downloads/FYP/deeplearning new/new-number-identification/crop/\"+'3.jpg',cell2)\n",
    "cv2.imwrite(\"C:/Users/zhaoh/Downloads/FYP/deeplearning new/new-number-identification/crop/\"+'4.jpg',cell3)\n",
    "cv2.imwrite(\"C:/Users/zhaoh/Downloads/FYP/deeplearning new/new-number-identification/crop/\"+'5.jpg',cell4)\n",
    "cv2.imwrite(\"C:/Users/zhaoh/Downloads/FYP/deeplearning new/new-number-identification/crop/\"+'6.jpg',cell5)\n",
    "cv2.imwrite(\"C:/Users/zhaoh/Downloads/FYP/deeplearning new/new-number-identification/crop/\"+'7.jpg',cell6)\n",
    "cv2.imwrite(\"C:/Users/zhaoh/Downloads/FYP/deeplearning new/new-number-identification/crop/\"+'8.jpg',cell7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"C:/Users/zhaoh/Downloads/FYP/deeplearning new/new-number-identification/crop/\"\n",
    "#\"C:/Users/zhaoh/Downloads/FYP/dataset/test/\"\n",
    "'''path='C:/Users/zhaoh/Downloads/FYP/dataset/test/'\n",
    "i=0\n",
    "for f in os.listdir(path):\n",
    "    name=os.path.join(path+str(f))\n",
    "    if \"jpg\" in name:\n",
    "        im = cv2.imread(name)\n",
    "        i+=1\n",
    "        flip = 255-im\n",
    "        cv2.imwrite(name,flip)\n",
    "print(i)\n",
    "'''\n",
    "test_set = DrivingDataset(data_dir=\"C:/Users/zhaoh/Downloads/FYP/dataset/test/\", is_train=False,transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=5,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "for i,j in testloader:\n",
    "    #i=255-i\n",
    "    print(i.shape)\n",
    "    for k in range(4):\n",
    "        img = np.moveaxis(i[k].numpy(),0,2)\n",
    "        #print(img.shape)\n",
    "        plt.imshow(img[:,:,0],cmap='gray')\n",
    "        plt.show()\n",
    "    i=i.reshape(-1,3,56,56)\n",
    "    outputs = model(i.to(device))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    #outputs = model(cell0.to(device))\n",
    "    #_, predicted = torch.max(outputs.data, 1)\n",
    "    #plt.show()\n",
    "    print(\"prediction\",predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(),'final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "confusion=[[0 for i in range(16)]for j in range(16)]\n",
    "for i,j in valloader:\n",
    "    #i=255-i\n",
    "    #print(i.shape)\n",
    "    target=j.squeeze(1)\n",
    "    target = target.long().to(device)\n",
    "    data = i[:,0]\n",
    "    data=data.reshape(-1,1,56,56).float().to(device)\n",
    "    outputs = model(data)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    #print(\"prediction\",predicted,target)\n",
    "    for k in range(len(predicted)):\n",
    "        #print(target[k].item(),' ',predicted[k].item())\n",
    "        confusion[ target[k].item()][predicted[k].item()]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0\n",
    "list=['0','1','2','3','4','5','6','7','8','9','+','-','*','/','=','.']\n",
    "print(list)\n",
    "for i in confusion:\n",
    "    print(i)\n",
    "    a+=sum(i)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#biggest mistake\n",
    "model.eval()\n",
    "test_set = DrivingDataset(data_dir=\"C:/Users/zhaoh/Downloads/FYP/dataset/eval/\", is_train=False,transform=transform2)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=10,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "\n",
    "a=[[]for i in range(16)]\n",
    "b=[[]for i in range(16)]\n",
    "lab=[[]for i in range(16)]\n",
    "\n",
    "for index,(k,j) in enumerate(testloader):\n",
    "    #i=255-i\n",
    "    #print(i.shape)\n",
    "    data = k[:,0]\n",
    "    data=data.reshape(-1,1,56,56).float().to(device)\n",
    "    target=j.squeeze(1)\n",
    "    target = target.to(device)\n",
    "    outputs = model(data)\n",
    "    score, predicted = torch.max(outputs.data, 1)\n",
    "    #print(\"prediction\",index)#batch result confidence \n",
    "    for i in range(len(score)):\n",
    "        if predicted[i]!=target[i].long():\n",
    "            #ni=ni+1\n",
    "            reg=target[i].item()\n",
    "            #print(target[i].item())\n",
    "            #print(a[reg])\n",
    "            #if score[i].item()>a[reg][0]:\n",
    "            a[reg].append(score[i].item())\n",
    "            b[reg].append(index*10+i)\n",
    "            lab[reg].append(predicted[i])\n",
    "    #for k in range(5):\n",
    "        #img = np.moveaxis(i[k].numpy(),0,2)\n",
    "        #print(img.shape)\n",
    "        #plt.imshow(img[:,:,0],cmap='gray')\n",
    "        #plt.show()\n",
    "    #outputs = model(cell0.to(device))\n",
    "    #_, predicted = torch.max(outputs.data, 1)\n",
    "    #print(predicted)\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "c=[]\n",
    "labe=[]\n",
    "for k in range(len(a)):\n",
    "    for i in range (5):\n",
    "        try:\n",
    "            index=a[k].index(heapq.nlargest(5,a[k])[i])\n",
    "            c.append(b[k][index])\n",
    "            labe.append(lab[k][index])\n",
    "        except:\n",
    "            print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1\n",
    "for i in range(len(c)):\n",
    "        n+=1\n",
    "        img = np.moveaxis(test_set[c[i]][0].numpy(),0,2)\n",
    "        plt.subplot(2,4,i%8+1)\n",
    "        plt.imshow(img)\n",
    "        print(labe[i])\n",
    "        #if i%8==0:\n",
    "        plt.show()\n",
    "        \n",
    "#print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_set = DrivingDataset(data_dir=\"C:/Users/zhaoh/Downloads/FYP/UI/master/sliced one/\", is_train=False,transform=transform2)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=1,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "for i,j in testloader:\n",
    "    #i=255-i\n",
    "    #print(i.shape)\n",
    "    data = i[:,0]\n",
    "    data=data.reshape(-1,1,56,56).float().to(device)\n",
    "    \n",
    "    outputs = model(data)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    print(\"prediction\",predicted)\n",
    "    #for k in range(5):\n",
    "        #img = np.moveaxis(i[k].numpy(),0,2)\n",
    "        #print(img.shape)\n",
    "        #plt.imshow(img[:,:,0],cmap='gray')\n",
    "        #plt.show()\n",
    "    #outputs = model(cell0.to(device))\n",
    "    #_, predicted = torch.max(outputs.data, 1)\n",
    "    #print(predicted)\n",
    "    #plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:/Users/zhaoh/Downloads/FYP/dataset/test/'\n",
    "i=0\n",
    "for f in os.listdir(path):\n",
    "    name=os.path.join(path+str(f))\n",
    "    if \"jpg\" in name:\n",
    "        im = cv2.imread(name)\n",
    "        plt.imshow(im)\n",
    "        plt.show()\n",
    "        im1=im[217:312,12:98]#86,4\n",
    "        im2=im[217:312,102:188]\n",
    "        im3=im[217:312,192:278]\n",
    "        im4=im[217:312,282:368]\n",
    "        im5=im[217:312,372:458]\n",
    "        im6=im[217:312,462:548]\n",
    "        im7=im[217:312,552:638]\n",
    "        im8=im[217:312,642:728]\n",
    "        im9=im[217:312,732:818]\n",
    "        im10=im[217:312,822:908]\n",
    "        im11=im[217:312,912:998]\n",
    "        #im5=im[870:990,420:575]\n",
    "        \n",
    "        plt.imshow(im11)\n",
    "        #i+=1\n",
    "        #im=255-im\n",
    "        #flip = cv2.flip(im, 1)\n",
    "        \n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im1)\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im2)\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im3)\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im4)\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im5)\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im6)\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im7)\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im8)\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im9) \n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im10)\n",
    "        cv2.imwrite(path+str(np.random.randint(10000,size=1).item())+'.jpg',im11)\n",
    "        \n",
    "#print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class visual(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(visual, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         # input shape (1, 28, 28)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              # input height\n",
    "                out_channels=16,            # n_filters\n",
    "                kernel_size=5,              # filter size\n",
    "                stride=2,                   # filter movement/step\n",
    "                padding=1,                  # if want same width and length of this image after con2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # output shape (16, 28, 28)\n",
    "            nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(16),# activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area, output shape (16, 14, 14)\n",
    "        )#32\n",
    "        self.conv2 = nn.Sequential(         # input shape (16, 14, 14)\n",
    "            nn.Conv2d(16, 32, 5, 2, 1),     # output shape (32, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(32),# activation\n",
    "            #nn.MaxPool2d(2),                # output shape (32, 7, 7)#16\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(         # input shape (16, 14, 14)\n",
    "            nn.Conv2d(32, 32, 5, 2, 1),     # output shape (32, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            #torch.nn.BatchNorm2d(32),# activation\n",
    "            #nn.MaxPool2d(2),                # output shape (32, 7, 7)#8\n",
    "        )\n",
    "        self.out1 = nn.Sequential(nn.Linear(32*36, 16),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        #self.out2 = nn.Linear(512, 16)\n",
    "        # fully connected layer, output 10 classes\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        output = x\n",
    "        \n",
    "        x = x.view(x.size(0), -1)   \n",
    "        print(x.shape)\n",
    "        x=self.out1(x)\n",
    "        \n",
    "        ## flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        \n",
    "        return output   # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class visual2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(visual2, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         # input shape (1, 28, 28)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              # input height\n",
    "                out_channels=16,            # n_filters\n",
    "                kernel_size=5,              # filter size\n",
    "                stride=1,                   # filter movement/step\n",
    "                padding=2,                  # if want same width and length of this image after con2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # output shape (16, 28, 28)\n",
    "            nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(16),# activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area, output shape (16, 14, 14)\n",
    "        )#32\n",
    "        self.conv2 = nn.Sequential(         # input shape (16, 14, 14)\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     # output shape (32, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(32),# activation\n",
    "            nn.MaxPool2d(2),                # output shape (32, 7, 7)#16\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(         # input shape (16, 14, 14)\n",
    "            nn.Conv2d(32, 32, 5, 1, 2),     # output shape (32, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(32),# activation\n",
    "            nn.MaxPool2d(2),                # output shape (32, 7, 7)#8\n",
    "        )\n",
    "        self.out1 = nn.Sequential(nn.Linear(32 * 7 * 7, 512),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.out2 = nn.Linear(512, 16)\n",
    "        # fully connected layer, output 10 classes\n",
    " \n",
    "    def forward(self, x):\n",
    "        x=x\n",
    "        print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print(x.shape)\n",
    "        output=x\n",
    "        x = self.conv2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        #print(x.shape)\n",
    "        #output=x\n",
    "        x = x.view(x.size(0), -1)   \n",
    "        x=self.out1(x)\n",
    "        #print(x.shape)\n",
    "        ## flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        output = self.out2(x)\n",
    "        return output   # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "o  torch.Size([1, 12544])\n",
      "o  torch.Size([16, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize\n",
    "\n",
    "#model=visual2()\n",
    "#model.to(device) \n",
    "model=torch.load('bestsimple.pkl')\n",
    "model.conv2 = nn.Dropout(0)\n",
    "model.conv3 = nn.Dropout(0)\n",
    "model.out1 = nn.Dropout(0)\n",
    "model.out2 = nn.Dropout(0)\n",
    "#print(model)\n",
    "model.eval()\n",
    "test_set = DrivingDataset(data_dir=\"C:/Users/zhaoh/Downloads/FYP/UI/master/sliced one/\", is_train=False,transform=transform2)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=1,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "for i,j in testloader:\n",
    "    #i=255-i\n",
    "    #print(i.shape)\n",
    "    data = i[:,0]\n",
    "    data=data.reshape(-1,1,56,56).float().to(device)\n",
    "    \n",
    "    outputs = model(data)\n",
    "    print('o ',outputs.shape)\n",
    "    channel=16\n",
    "    size=28\n",
    "    outputs=np.reshape(outputs.cpu().detach(),(channel,size,size))\n",
    "    print('o ',outputs.shape)\n",
    "    #_, predicted = torch.max(outputs.data, 1)\n",
    "    #print(\"prediction\",predicted)\n",
    "    \n",
    "    for k in range(channel):\n",
    "        plt.subplot(2,4,k%8+1)\n",
    "        #img = np.moveaxis(i[:,k].numpy(),0,2)\n",
    "        print(outputs[k].shape)\n",
    "        img=np.reshape(outputs[k].cpu().detach(),(size,size))\n",
    "        #img = np.moveaxis(i[:,k].numpy(),0,2)\n",
    "        plt.imshow(img,cmap='gray')\n",
    "        if k%8==7:\n",
    "            plt.show()\n",
    "    \n",
    "    #outputs = model(cell0.to(device))\n",
    "    #_, predicted = torch.max(outputs.data, 1)\n",
    "    #print(predicted)\n",
    "    #plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "torch.Size([1, 1, 56, 56])\n",
      "torch.Size([1, 16, 28, 28])\n",
      "torch.Size([1, 32, 14, 14])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 1, 56, 56])\n",
      "torch.Size([1, 16, 28, 28])\n",
      "torch.Size([1, 32, 14, 14])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 1, 56, 56])\n",
      "torch.Size([1, 16, 28, 28])\n",
      "torch.Size([1, 32, 14, 14])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 1, 56, 56])\n",
      "torch.Size([1, 16, 28, 28])\n",
      "torch.Size([1, 32, 14, 14])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 1, 56, 56])\n",
      "torch.Size([1, 16, 28, 28])\n",
      "torch.Size([1, 32, 14, 14])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 1, 56, 56])\n",
      "torch.Size([1, 16, 28, 28])\n",
      "torch.Size([1, 32, 14, 14])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 1, 56, 56])\n",
      "torch.Size([1, 16, 28, 28])\n",
      "torch.Size([1, 32, 14, 14])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 1, 56, 56])\n",
      "torch.Size([1, 16, 28, 28])\n",
      "torch.Size([1, 32, 14, 14])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 1, 56, 56])\n",
      "torch.Size([1, 16, 28, 28])\n",
      "torch.Size([1, 32, 14, 14])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 1, 56, 56])\n",
      "torch.Size([1, 16, 28, 28])\n",
      "torch.Size([1, 32, 14, 14])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 1, 56, 56])\n",
      "torch.Size([1, 16, 28, 28])\n",
      "torch.Size([1, 32, 14, 14])\n",
      "torch.Size([1, 32, 7, 7])\n",
      "torch.Size([1, 32, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "model=visual2()\n",
    "#print(model)\n",
    "model.to(device)    \n",
    "model.eval()\n",
    "test_set = DrivingDataset(data_dir=\"C:/Users/zhaoh/Downloads/FYP/UI/master/sliced one/\", is_train=False,transform=transform2)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=1,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "for i,j in testloader:\n",
    "    #i=255-i\n",
    "    #print(i.shape)\n",
    "    data = i[:,0]\n",
    "    data=data.reshape(-1,1,56,56).float().to(device)\n",
    "    \n",
    "    outputs = model(data)\n",
    "    print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "a='1568778798'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,len(a)):\n",
    "    a=a.replace(a[i],'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
